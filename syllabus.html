<!DOCTYPE html>
<html>
	<head>
		<link rel="stylesheet" type="text/css" href="/css/main.css">
	</head>	

	<body>
	  	<div id="syllabus">
		    <h1>Syllabus</h1>
		    <ul class="posts">
		      <li>Course Textbook (free download) &rarr; <a href="https://web.stanford.edu/~hastie/local.ftp/Springer/ISLR_print1.pdf">An Introduction to Statistical Learning (ISLR)</a></li>
		    </ul>
		    <ul class="posts">
		      <li>--------------------------------------</li>
		      <li>Materials to study each evening</li>
		      <li>--------------------------------------</li>
		    </ul>
		      <li><span>June 19th (Intro & Nearest Neighbors)</span> &rarr; <a href="http://karpathy.github.io/2012/10/22/state-of-computer-vision/">The State of AI</a><span> & </span><a href="https://www.youtube.com/watch?v=UqYde-LULfs">Nearest Neighbors</a> <span> & ISLR Chapter 2: Read to understand all sections covering the KNN algorithm </span></li>
		      <li><span>June 20th (KNN & Cross-Validation)</span> &rarr; <span>ISLR Chapter 3: Read to understand all sections covering the KNN algorithm & ISLR Chapter 5.1: Read to understand cross-validation</span></li>
		      <li><span>June 21st (Regression)</span> &rarr; <span>ISLR Chapters 3 and 4: Read to understand Linear Regression and Logistic Regression</span></li>
		      <li><span>June 22nd (Scikit Learn, Softmax & Loss Functions)</span> &rarr; <a href="http://peterroelants.github.io/posts/neural_network_implementation_intermezzo02/">Read this softmax tutorial with the help of your TAs</a></li>
		      <li><span>June 23rd (Gradient Descent & Intro to Neural Networks)</span> &rarr; <a href="https://spin.atomicobject.com/2014/06/24/gradient-descent-linear-regression/">Read this Gradient Descent tutorial with the help of your TAs</a></li> 
		</div>
	</body>
</html>
